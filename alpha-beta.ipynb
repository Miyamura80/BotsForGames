{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyspiel\n",
    "import copy\n",
    "\n",
    "BOARD_SIZE = 3\n",
    "game = pyspiel.load_game(\"hex\",{\"board_size\":BOARD_SIZE})\n",
    "BLACK, WHITE = 1, -1  # first turn or second turn player\n",
    "\n",
    "class State:\n",
    "    '''Board implementation of BOARD_SIZE x BOARD_SIZE Hex Board'''\n",
    "    X, Y = 'ABCDEFGHI'[0:BOARD_SIZE],  '123456789'[0:BOARD_SIZE]\n",
    "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE)) # (x, y)\n",
    "        self.color = 1\n",
    "        self.win_color = 0\n",
    "        self.record = []\n",
    "        self.hex_state = game.new_initial_state()\n",
    "\n",
    "    def action2str(self, a: int):\n",
    "        return self.X[a // BOARD_SIZE] + self.Y[a % BOARD_SIZE]\n",
    "\n",
    "    def str2action(self, s: str):\n",
    "        return self.X.find(s[0]) * BOARD_SIZE + self.Y.find(s[1])\n",
    "\n",
    "    def record_string(self):\n",
    "        return ' '.join([self.action2str(a) for a in self.record])\n",
    "    \n",
    "    def __deepcopy__(self):\n",
    "        newState = State()\n",
    "        newState.board = copy.deepcopy(self.board)\n",
    "        newState.win_color = copy.deepcopy(self.win_color)\n",
    "        newState.record = copy.deepcopy(self.record)\n",
    "        newState.hex_state = copy.deepcopy(self.hex_state)\n",
    "        return newState\n",
    "\n",
    "    def __str__(self):\n",
    "        final_bd = [\" \"+\" \".join(self.Y)]\n",
    "        hex_bd = str(self.hex_state).split(\"\\n\")\n",
    "        for i in range(len(hex_bd)):\n",
    "            final_bd.append(self.X[i]+\" \"+hex_bd[i])\n",
    "        return \"\\n\".join(final_bd)\n",
    "\n",
    "    def dry_play(self, action):\n",
    "        freshState = self.__deepcopy__()\n",
    "        freshState.play(action)\n",
    "        return freshState\n",
    "\n",
    "    def play(self, action):\n",
    "        # state transition function\n",
    "        # action is position interger (0~8) or string representation of action sequence\n",
    "        # Handles the case where action is sequence of actions \"0 1 2 3 4\"\n",
    "        if isinstance(action, str):\n",
    "            for astr in action.split():\n",
    "                self.play(self.str2action(astr))\n",
    "            return self\n",
    "\n",
    "        # Single action case\n",
    "        x, y = action // BOARD_SIZE, action % BOARD_SIZE\n",
    "        self.board[x, y] = self.color\n",
    "        self.hex_state.apply_action(action)\n",
    "\n",
    "        # check whether 3 stones are on the line\n",
    "        if self.hex_state.is_terminal():\n",
    "            self.win_color = self.color\n",
    "\n",
    "        self.color = -self.color\n",
    "        self.record.append(action)\n",
    "        return self\n",
    "\n",
    "    def terminal(self):\n",
    "        # terminal state check\n",
    "        return self.hex_state.is_terminal()\n",
    "\n",
    "    def terminal_reward(self):\n",
    "        # terminal reward \n",
    "        # return self.win_color if self.color == BLACK else -self.win_color\n",
    "        return self.win_color\n",
    "\n",
    "    def legal_actions(self):\n",
    "        # list of legal actions on each state\n",
    "        return [a for a in range(BOARD_SIZE * BOARD_SIZE) if self.board[a // BOARD_SIZE, a % BOARD_SIZE] == 0]\n",
    "\n",
    "    def feature(self):\n",
    "        # input tensor for neural net (state)\n",
    "        # return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
    "        observation =  np.array(self.hex_state.observation_tensor(), np.float32)\n",
    "        return observation.reshape(9,BOARD_SIZE,BOARD_SIZE)\n",
    "\n",
    "    def action_feature(self, action):\n",
    "        # input tensor for neural net (action)\n",
    "        a = np.zeros((1, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n",
    "        a[0, action // BOARD_SIZE, action % BOARD_SIZE] = 1\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import math\n",
    "\n",
    "class MiniMaxAgent:\n",
    "    def __init__(self):\n",
    "        self.result = defaultdict(lambda x: defaultdict(int))\n",
    "    \n",
    "    def think(self, state: State, sim_num: int, temperature:int, show=False):\n",
    "        return self.minimax(state, BOARD_SIZE*BOARD_SIZE, -math.inf, math.inf, state.color==1)\n",
    "\n",
    "    def minimax(self, state, depth, alpha, beta, maximizing_player):\n",
    "\n",
    "        if depth == 0 or state.terminal():\n",
    "            return None, state.terminal_reward()\n",
    "\n",
    "        children = state.legal_actions()\n",
    "        best_move = children[0]\n",
    "        \n",
    "        if maximizing_player:\n",
    "            max_eval = -math.inf        \n",
    "            for child in children:\n",
    "                freshState = state.__deepcopy__()\n",
    "                freshState.play(child)\n",
    "                current_eval = self.minimax(freshState, depth - 1, alpha, beta, False)[1]\n",
    "                if current_eval > max_eval:\n",
    "                    max_eval = current_eval\n",
    "                    best_move = child\n",
    "                alpha = max(alpha, current_eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return best_move, max_eval\n",
    "\n",
    "        else:\n",
    "            min_eval = math.inf\n",
    "            for child in children:\n",
    "                freshState = state.__deepcopy__()\n",
    "                freshState.play(child)\n",
    "                current_eval = self.minimax(freshState, depth - 1, alpha, beta, True)[1]\n",
    "                if current_eval < min_eval:\n",
    "                    min_eval = current_eval\n",
    "                    best_move = child\n",
    "                beta = min(beta, current_eval)\n",
    "                if beta <= alpha:\n",
    "                    break\n",
    "            return best_move, min_eval\n",
    "\n",
    "    def my_minimax(self, state: State):\n",
    "        if str(state) in self.result:\n",
    "            return self.result[str(state)]\n",
    "\n",
    "        if state.terminal():\n",
    "            self.result[str(state)] = state.terminal_reward()\n",
    "            return state.terminal_reward()\n",
    "        else:\n",
    "            results = []\n",
    "            for a in state.legal_actions():\n",
    "                freshState = state.__deepcopy__()\n",
    "                freshState.play(a)\n",
    "                value = self.my_minimax(freshState)\n",
    "                self.result[str(freshState)] = value\n",
    "                results.append((a, value))\n",
    "            if state.color==1:\n",
    "                maxPair = max(results, key=lambda x: x[1])\n",
    "                return maxPair[0]\n",
    "            else:\n",
    "                minPair = min(results, key=lambda x: x[1])\n",
    "                return minPair[0]\n",
    "\n",
    "    def pv(self, state):\n",
    "        nextStates = [(a,self.result[str(state.dry_play(a))]) for a in state.legal_actions()]\n",
    "        return [max(nextStates, key=lambda x: x[1])[1] ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 2 3\n",
      "A y . . \n",
      "B  . . . \n",
      "C   . . . \n",
      " 1 2 3\n",
      "A y y . \n",
      "B  . o . \n",
      "C   . . . \n",
      " 1 2 3\n",
      "A y y y \n",
      "B  p p . \n",
      "C   . . . \n",
      " 1 2 3\n",
      "A y y y \n",
      "B  p p y \n",
      "C   . . q \n",
      " 1 2 3\n",
      "A y y y \n",
      "B  p p y \n",
      "C   . O q \n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Search with trained net\n",
    "\n",
    "# tree = Tree(net)\n",
    "tree = MiniMaxAgent()\n",
    "state = State()\n",
    "while True:  \n",
    "  move, eval = tree.think(state, 5000, temperature=1, show=True)\n",
    "  # pv_seq = tree.pv(state)\n",
    "  # print(pv_seq)\n",
    "  state.play(move)\n",
    "  # display_heatmap2(distb.reshape((BOARD_SIZE,BOARD_SIZE)))\n",
    "  print(state)\n",
    "  if state.terminal():\n",
    "    break\n",
    "  noCorrectMove = True\n",
    "  while noCorrectMove:\n",
    "    user_input = input(\"Input move: \")\n",
    "    if state.str2action(user_input) in state.legal_actions():\n",
    "      noCorrectMove = False\n",
    "  state.play(user_input)\n",
    "  if state.terminal():\n",
    "    break\n",
    "print(state)\n",
    "print(state.terminal_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 6)\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gl = [(1,2),(3,4),(5,6)]\n",
    "f = lambda x: x[0]\n",
    "print(max(gl, key=lambda x: x[1]))\n",
    "print(f((3,4)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
