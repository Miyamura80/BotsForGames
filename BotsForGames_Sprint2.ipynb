{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miyamura80/BotsForGames/blob/main/BotsForGames_Sprint2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0DCN2-Op6F2",
        "outputId": "69bf5846-f956-41e2-b078-29bedcce8fbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.1.1)\n",
            "Collecting open_spiel\n",
            "  Downloading open_spiel-1.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting scipy>=1.5.4\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 297 kB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (1.0.0)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (21.4.0)\n",
            "Requirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (21.1.3)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.10.0->open_spiel) (1.15.0)\n",
            "Installing collected packages: scipy, open-spiel\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed open-spiel-1.1.0 scipy-1.7.3\n"
          ]
        }
      ],
      "source": [
        "# environment:\n",
        "!pip3 install torch\n",
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prmwuNkmWU5g"
      },
      "source": [
        "# Implementation of hex board state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycSlk7q8p6F4",
        "outputId": "253d4161-ecdf-49c0-b79a-2bd9d1f9b807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 1 2 3 4 5 6 7\n",
            "A . . . . . . . \n",
            "B  x . . . . . . \n",
            "C   . . . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pyspiel\n",
        "import copy\n",
        "\n",
        "BOARD_SIZE = 3\n",
        "game = pyspiel.load_game(\"hex\",{\"board_size\":BOARD_SIZE})\n",
        "BLACK, WHITE = 1, -1  # first turn or second turn player\n",
        "\n",
        "class State:\n",
        "    '''Board implementation of BOARD_SIZE x BOARD_SIZE Hex Board'''\n",
        "    X, Y = 'ABCDEFGHI'[0:BOARD_SIZE],  '123456789'[0:BOARD_SIZE]\n",
        "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE)) # (x, y)\n",
        "        self.color = 1\n",
        "        self.win_color = 0\n",
        "        self.record = []\n",
        "        self.hex_state = game.new_initial_state()\n",
        "\n",
        "    def __deepcopy__(self):\n",
        "        newState = State()\n",
        "        newState.board = copy.deepcopy(self.board)\n",
        "        newState.win_color = copy.deepcopy(self.win_color)\n",
        "        newState.record = copy.deepcopy(self.record)\n",
        "        newState.hex_state = copy.deepcopy(self.hex_state)\n",
        "        return newState\n",
        "\n",
        "    def action2str(self, a: int):\n",
        "        return self.X[a // BOARD_SIZE] + self.Y[a % BOARD_SIZE]\n",
        "\n",
        "    def str2action(self, s: str):\n",
        "        return self.X.find(s[0]) * BOARD_SIZE + self.Y.find(s[1])\n",
        "\n",
        "    def record_string(self):\n",
        "        return ' '.join([self.action2str(a) for a in self.record])\n",
        "\n",
        "    def __str__(self):\n",
        "        final_bd = [\" \"+\" \".join(self.Y)]\n",
        "        hex_bd = str(self.hex_state).split(\"\\n\")\n",
        "        for i in range(len(hex_bd)):\n",
        "            final_bd.append(self.X[i]+\" \"+hex_bd[i])\n",
        "        return \"\\n\".join(final_bd)\n",
        "\n",
        "    def play(self, action):\n",
        "        # state transition function\n",
        "        # action is position interger (0~8) or string representation of action sequence\n",
        "        # Handles the case where action is sequence of actions \"0 1 2 3 4\"\n",
        "        if isinstance(action, str):\n",
        "            for astr in action.split():\n",
        "                self.play(self.str2action(astr))\n",
        "            return self\n",
        "\n",
        "        # Single action case\n",
        "        x, y = action // BOARD_SIZE, action % BOARD_SIZE\n",
        "        self.board[x, y] = self.color\n",
        "        self.hex_state.apply_action(action)\n",
        "\n",
        "        # check whether 3 stones are on the line\n",
        "        if self.hex_state.is_terminal():\n",
        "            self.win_color = self.color\n",
        "\n",
        "        self.color = -self.color\n",
        "        self.record.append(action)\n",
        "        return self\n",
        "\n",
        "    def terminal(self):\n",
        "        # terminal state check\n",
        "        return self.hex_state.is_terminal()\n",
        "\n",
        "    def terminal_reward(self):\n",
        "        # terminal reward \n",
        "        # return self.win_color if self.color == BLACK else -self.win_color\n",
        "        return self.win_color\n",
        "\n",
        "    def legal_actions(self):\n",
        "        # list of legal actions on each state\n",
        "        return [a for a in range(BOARD_SIZE * BOARD_SIZE) if self.board[a // BOARD_SIZE, a % BOARD_SIZE] == 0]\n",
        "\n",
        "    def feature(self):\n",
        "        # input tensor for neural net (state)\n",
        "        # return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
        "        observation =  np.array(self.hex_state.observation_tensor(), np.float32)\n",
        "        return observation.reshape(9,BOARD_SIZE,BOARD_SIZE)\n",
        "\n",
        "    def action_feature(self, action):\n",
        "        # input tensor for neural net (action)\n",
        "        a = np.zeros((1, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n",
        "        a[0, action // BOARD_SIZE, action % BOARD_SIZE] = 1\n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhwWeH5GWc5N"
      },
      "source": [
        "# Small neural nets with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4TlYnoxp6GA",
        "outputId": "21c54c47-549d-471d-a169-0177c36af8aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Small neural nets with PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# convnet from input channels w number filters0, output channels w number filters1\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
        "        self.bn = None\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(filters1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            h = self.bn(h)\n",
        "        return h\n",
        "\n",
        "# 3x3 conv filter with same input & output channels \n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, filters):\n",
        "        super().__init__()\n",
        "        self.conv = Conv(filters, filters, 3, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(x + (self.conv(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhPImlQ1WhOD"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUYX23urp6GC"
      },
      "outputs": [],
      "source": [
        "num_filters = 16\n",
        "num_blocks = 4\n",
        "\n",
        "class Representation(nn.Module):\n",
        "    ''' Conversion from observation to inner abstract state '''\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
        "\n",
        "        self.layer0 = Conv(self.input_shape[0], num_filters, 7, bn=True)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.layer0(x))\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "    def inference(self, x):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            rp = self(torch.from_numpy(x).unsqueeze(0).to('cuda'))\n",
        "        return rp.cpu().numpy()[0]\n",
        "\n",
        "class Prediction(nn.Module):\n",
        "    ''' Policy and value prediction from inner abstract state '''\n",
        "    def __init__(self, action_shape):\n",
        "        super().__init__()\n",
        "        self.board_size = np.prod(action_shape[1:])\n",
        "        self.action_size = action_shape[0] * self.board_size\n",
        "\n",
        "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.conv_p2 = Conv(4, 1, 1)\n",
        "\n",
        "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
        "\n",
        "    def forward(self, rp):\n",
        "        h_p = F.relu(self.conv_p1(rp))\n",
        "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
        "\n",
        "        h_v = F.relu(self.conv_v(rp))\n",
        "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
        "\n",
        "        # range of value is -1 ~ 1\n",
        "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
        "\n",
        "    def inference(self, rp):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p, v = self(torch.from_numpy(rp).unsqueeze(0).to('cuda'))\n",
        "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]\n",
        "\n",
        "class Dynamics(nn.Module):\n",
        "    '''Abstract state transition'''\n",
        "    def __init__(self, rp_shape, act_shape):\n",
        "        super().__init__()\n",
        "        self.rp_shape = rp_shape\n",
        "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, rp, a):\n",
        "        h = torch.cat([rp, a], dim=1)\n",
        "        h = self.layer0(h)\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "    def inference(self, rp, a):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            rp = self(torch.from_numpy(rp).unsqueeze(0).to('cuda'), torch.from_numpy(a).unsqueeze(0).to('cuda'))\n",
        "        return rp.cpu().numpy()[0]\n",
        "\n",
        "''' Collects all h, f, g functions in one net, and predict outcome ''' \n",
        "class Net(nn.Module):\n",
        "    '''Whole net'''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        state = State()\n",
        "        input_shape = state.feature().shape\n",
        "        print(input_shape)\n",
        "        action_shape = state.action_feature(0).shape\n",
        "        rp_shape = (num_filters, *input_shape[1:])\n",
        "\n",
        "        self.representation = Representation(input_shape)\n",
        "        self.prediction = Prediction(action_shape)\n",
        "        self.dynamics = Dynamics(rp_shape, action_shape)\n",
        "\n",
        "    def predict(self, state0, path):\n",
        "        '''\n",
        "        Predict p and v from original state and path\n",
        "        Return [(p1,v1), (p2,v2), ...]\n",
        "        Where path :: [Action]\n",
        "        '''\n",
        "        \n",
        "        outputs = []\n",
        "        x = state0.feature()\n",
        "        rp = self.representation.inference(x)\n",
        "        outputs.append(self.prediction.inference(rp))\n",
        "        for action in path:\n",
        "            a = state0.action_feature(action)\n",
        "            rp = self.dynamics.inference(rp, a)\n",
        "            outputs.append(self.prediction.inference(rp))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nq-MFjop6GE",
        "outputId": "73880283-466e-4226-aad3-0e6bba2956a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7, 7, 7)\n",
            " 1 2 3 4 5 6 7\n",
            "A . . . . . . . \n",
            "B  . . . . . . . \n",
            "C   . . . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n",
            "p = \n",
            "[[[20 20 20 20 20 20 20]\n",
            "  [19 19 19 19 19 20 20]\n",
            "  [20 20 20 20 20 20 20]\n",
            "  [20 20 20 20 20 20 20]\n",
            "  [20 20 20 20 20 20 20]\n",
            "  [20 20 20 20 21 20 20]\n",
            "  [20 20 20 19 20 19 19]]]\n",
            "v =  -0.012665885\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def show_net(net, state):\n",
        "    '''Display policy (p) and value (v)'''\n",
        "    print(state)\n",
        "    p, v = net.predict(state, [])[-1]\n",
        "    print('p = ')\n",
        "    print((p * 1000).astype(int).reshape((-1, *net.representation.input_shape[1:3])))\n",
        "    print('v = ', v)\n",
        "    print()\n",
        "\n",
        "#  Outputs before training\n",
        "show_net(Net().to('cuda'), State())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekC6XGI4WQ--"
      },
      "source": [
        "# Implementation of Monte Carlo Tree Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqlkZ8MCp6GE"
      },
      "outputs": [],
      "source": [
        "# Implementation of Monte Carlo Tree Search\n",
        "\n",
        "class Node:\n",
        "    '''Search result of one abstract (or root) state'''\n",
        "    def __init__(self, p, v):#\n",
        "        ''' Policy and Value '''\n",
        "        self.p, self.v = p, v\n",
        "        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)\n",
        "        self.n_all, self.q_sum_all = 1, v / 2 # prior\n",
        "\n",
        "    def update(self, action, q_new):\n",
        "        # Update\n",
        "        self.n[action] += 1\n",
        "        self.q_sum[action] += q_new\n",
        "\n",
        "        # Update overall stats\n",
        "        self.n_all += 1\n",
        "        self.q_sum_all += q_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMuA1Ednp6GF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "class Tree:\n",
        "    '''Monte Carlo Tree'''\n",
        "    def __init__(self, net):\n",
        "        self.net = net\n",
        "        self.nodes = {}\n",
        "\n",
        "    def search(self, state, path, rp, depth):\n",
        "        # Return predicted value from new state\n",
        "        key = state.record_string()\n",
        "        if len(path) > 0:\n",
        "            key += '|' + ' '.join(map(state.action2str, path))\n",
        "        if key not in self.nodes:\n",
        "            p, v = self.net.prediction.inference(rp)\n",
        "            self.nodes[key] = Node(p, v)\n",
        "            return v\n",
        "\n",
        "        # State transition by an action selected from bandit\n",
        "        node = self.nodes[key]\n",
        "        p = node.p\n",
        "        mask = np.zeros_like(p)\n",
        "        if depth == 0:\n",
        "            # Add noise to policy on the root node\n",
        "            p = 0.75 * p + 0.25 * np.random.dirichlet([0.15] * len(p))\n",
        "            # On the root node, we choose action only from legal actions\n",
        "            mask[state.legal_actions()] = 1\n",
        "            p *= mask\n",
        "            p /= p.sum() + 1e-16\n",
        "\n",
        "        n, q_sum = 1 + node.n, node.q_sum_all / node.n_all + node.q_sum\n",
        "        ucb = q_sum / n + 2.0 * np.sqrt(node.n_all) * p / n + mask * 4 # PUCB formula\n",
        "        best_action = np.argmax(ucb)\n",
        "\n",
        "        # Search next state by recursively calling this function\n",
        "        rp_next = self.net.dynamics.inference(rp, state.action_feature(best_action))\n",
        "        path.append(best_action)\n",
        "        q_new = -self.search(state, path, rp_next, depth + 1) # With the assumption of changing player by turn\n",
        "        node.update(best_action, q_new)\n",
        "\n",
        "        return q_new\n",
        "\n",
        "    def think(self, state, num_simulations, temperature = 0, show=False):\n",
        "        # End point of MCTS\n",
        "        if show:\n",
        "            print(state)\n",
        "        start, prev_time = time.time(), 0\n",
        "        for _ in range(num_simulations):\n",
        "            self.search(state, [], self.net.representation.inference(state.feature()), depth=0)\n",
        "\n",
        "            # Display search result on every second\n",
        "            if show:\n",
        "                tmp_time = time.time() - start\n",
        "                if int(tmp_time) > int(prev_time):\n",
        "                    prev_time = tmp_time\n",
        "                    root, pv = self.nodes[state.record_string()], self.pv(state)\n",
        "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'\n",
        "                          % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],\n",
        "                             root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))\n",
        "\n",
        "        #  Return probability distribution weighted by the number of simulations\n",
        "        root = self.nodes[state.record_string()]\n",
        "        n = root.n + 1\n",
        "        n = (n / np.max(n)) ** (1 / (temperature + 1e-8))\n",
        "        return n / n.sum()\n",
        "\n",
        "    def pv(self, state):\n",
        "        # Return principal variation (action sequence which is considered as the best)\n",
        "        s, pv_seq = copy.deepcopy(state), []\n",
        "        while True:\n",
        "            key = s.record_string()\n",
        "            if key not in self.nodes or self.nodes[key].n.sum() == 0:\n",
        "                break\n",
        "            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]\n",
        "            pv_seq.append(best_action)\n",
        "            s.play(best_action)\n",
        "        return pv_seq\n",
        "    \n",
        "    def values(self, state):\n",
        "        s, pv_seq = copy.deepcopy(state), []\n",
        "        key = s.record_string()\n",
        "        if key not in self.nodes:\n",
        "            return np.zeros(49).reshape((7,7))\n",
        "\n",
        "        result = np.zeros(49)\n",
        "        best_actions = [(a, self.nodes[key].n[a]) for a in s.legal_actions()]\n",
        "        for (a, v) in best_actions:\n",
        "            result[a] = v\n",
        "        return result.reshape((7,7))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGrCluKrp6GG",
        "outputId": "2166ba91-ace2-420e-cbaf-6e025a5e149c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7, 7, 7)\n",
            " 1 2 3 4 5 6 7\n",
            "A . . . . . . . \n",
            "B  . . . . . . . \n",
            "C   . . . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n",
            "1.02 sec. best D2. q = -0.0044. n = 3 / 68. pv = D2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[2., 2., 2., 2., 2., 1., 1.],\n",
              "       [2., 3., 3., 2., 1., 2., 2.],\n",
              "       [1., 2., 2., 2., 2., 2., 2.],\n",
              "       [2., 3., 2., 2., 2., 2., 3.],\n",
              "       [1., 2., 2., 3., 3., 3., 1.],\n",
              "       [2., 3., 2., 2., 1., 2., 3.],\n",
              "       [2., 2., 1., 2., 2., 2., 2.]])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Search with initialized net\n",
        "\n",
        "tree = Tree(Net().to('cuda'))\n",
        "\n",
        "tree.think(State(), 100, show=True)\n",
        "tree.values(State())\n",
        "\n",
        "# tree = Tree(Net().to('cuda'))\n",
        "# tree.think(State().play('A1 C1 A2 C2'), 200, show=True)\n",
        "\n",
        "# tree = Tree(Net().to('cuda'))\n",
        "# tree.think(State().play('B2 A2 A3 C1 B3'), 200, show=True)\n",
        "\n",
        "# tree = Tree(Net().to('cuda'))\n",
        "# tree.think(State().play('B2 A2 A3 C1'), 200, show=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0anzRbZYV718"
      },
      "source": [
        "# Training of neural net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGJtw40qp6GH"
      },
      "outputs": [],
      "source": [
        "# Training of neural net\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "batch_size = 32\n",
        "num_steps = 100\n",
        "\n",
        "def gen_target(ep, k):\n",
        "    '''Generate inputs and targets for training'''\n",
        "    # path, reward, observation, action, policy\n",
        "    turn_idx = np.random.randint(len(ep[0]))\n",
        "    ps, vs, ax = [], [], []\n",
        "    for t in range(turn_idx, turn_idx + k + 1):\n",
        "        if t < len(ep[0]):\n",
        "            p = ep[4][t]\n",
        "            a = ep[3][t]\n",
        "        else: # state after finishing game\n",
        "            # p is 0 (loss is 0)\n",
        "            p = np.zeros_like(ep[4][-1])\n",
        "            # random action selection\n",
        "            a = np.zeros(np.prod(ep[3][-1].shape), dtype=np.float32)\n",
        "            a[np.random.randint(len(a))] = 1\n",
        "            a = a.reshape(ep[3][-1].shape)\n",
        "        vs.append([ep[1] if t % 2 == 0 else -ep[1]])\n",
        "        ps.append(p)\n",
        "        ax.append(a)\n",
        "        \n",
        "    return ep[2][turn_idx], ax, ps, vs\n",
        "\n",
        "def train(episodes, net, opt):\n",
        "    '''Train neural net'''\n",
        "    p_loss_sum, v_loss_sum = 0, 0\n",
        "    net.train()\n",
        "    k = 4\n",
        "    for _ in range(num_steps):\n",
        "        x, ax, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))], k) for j in range(batch_size)])\n",
        "        x = torch.from_numpy(np.array(x)).to('cuda')\n",
        "        ax = torch.from_numpy(np.array(ax)).to('cuda')\n",
        "        p_target = torch.from_numpy(np.array(p_target)).to('cuda')\n",
        "        v_target = torch.FloatTensor(np.array(v_target)).to('cuda')\n",
        "\n",
        "        # Change the order of axis as [time step, batch, ...]\n",
        "        ax = torch.transpose(ax, 0, 1)\n",
        "        p_target = torch.transpose(p_target, 0, 1)\n",
        "        v_target = torch.transpose(v_target, 0, 1)\n",
        "\n",
        "        # Compute losses for k (+ current) steps\n",
        "        p_loss, v_loss = 0, 0\n",
        "        for t in range(k + 1):\n",
        "            rp = net.representation(x) if t == 0 else net.dynamics(rp, ax[t - 1])\n",
        "            p, v = net.prediction(rp)\n",
        "            p_loss += F.kl_div(torch.log(p), p_target[t], reduction='sum')\n",
        "            v_loss += torch.sum(((v_target[t] - v) ** 2) / 2)\n",
        "\n",
        "        p_loss_sum += p_loss.item()\n",
        "        v_loss_sum += v_loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        (p_loss + v_loss).backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    num_train_datum = num_steps * batch_size\n",
        "    print('p_loss %f v_loss %f' % (p_loss_sum / num_train_datum, v_loss_sum / num_train_datum))\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dhDo47mV_dj"
      },
      "source": [
        "#  Battle against random agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRo8A-WTp6GI"
      },
      "outputs": [],
      "source": [
        "#  Battle against random agents\n",
        "\n",
        "def vs_random(net, n=100):\n",
        "    results = {}\n",
        "    for i in range(n):\n",
        "        first_turn = i % 2 == 0\n",
        "        turn = first_turn\n",
        "        state = State()\n",
        "        while not state.terminal():\n",
        "            if turn:\n",
        "                p, _ = net.predict(state, [])[-1]\n",
        "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\n",
        "            else:\n",
        "                action = np.random.choice(state.legal_actions())\n",
        "            state.play(action)\n",
        "            turn = not turn\n",
        "        r = state.terminal_reward() if turn else -state.terminal_reward()\n",
        "        results[r] = results.get(r, 0) + 1\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XztEthigVxVT"
      },
      "source": [
        "# Main Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XviavCidp6GJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Main algorithm of MuZero\n",
        "\n",
        "# num_games = 50000\n",
        "# num_games_one_epoch = 20\n",
        "# num_simulations = 40\n",
        "\n",
        "# net = Net()\n",
        "# net.load_state_dict(torch.load(\"/home/eito/Github/BotsForGames/network.pkl\"))\n",
        "\n",
        "# net.to('cuda')\n",
        "# optimizer = optim.SGD(net.parameters(), lr=3e-4, weight_decay=3e-5, momentum=0.8)\n",
        "\n",
        "# # Display battle results as {-1: lose 0: draw 1: win} (for episode generated for training, 1 means that the first player won)\n",
        "# vs_random_sum = vs_random(net)\n",
        "# print('vs_random = ', sorted(vs_random_sum.items()))\n",
        "\n",
        "# episodes = []\n",
        "# result_distribution = {1: 0, 0: 0, -1: 0}\n",
        "\n",
        "# for g in range(num_games):\n",
        "#     # Generate one episode\n",
        "#     record, p_targets, features, action_features = [], [], [], []\n",
        "#     state = State()\n",
        "#     # temperature using to make policy targets from search results\n",
        "#     temperature = 0.7\n",
        "\n",
        "#     while not state.terminal():\n",
        "#         tree = Tree(net)\n",
        "#         p_target = tree.think(state, num_simulations, temperature)\n",
        "#         p_targets.append(p_target)\n",
        "#         features.append(state.feature())\n",
        "\n",
        "#         # Select action with generated distribution, and then make a transition by that action\n",
        "#         p_target = [p_target[i] if i in state.legal_actions() else 0 for i in range(len(p_target))]\n",
        "#         p_target = np.array(p_target) / sum(p_target)\n",
        "#         action = np.random.choice(np.arange(len(p_target)), p=p_target)\n",
        "#         record.append(action)\n",
        "#         action_features.append(state.action_feature(action))\n",
        "#         state.play(action)\n",
        "#         temperature *= 0.8\n",
        "\n",
        "#     # reward seen from the first turn player\n",
        "#     reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\n",
        "#     result_distribution[reward] += 1\n",
        "#     episodes.append((record, reward, features, action_features, p_targets))\n",
        "\n",
        "#     if g % num_games_one_epoch == 0:\n",
        "#         print('game ', end='')\n",
        "#     print(g, ' ', end='')\n",
        "\n",
        "#     # Training of neural net\n",
        "#     if (g + 1) % num_games_one_epoch == 0:\n",
        "#         # Show the result distributiuon of generated episodes\n",
        "#         print('generated = ', sorted(result_distribution.items()))\n",
        "#         net = train(episodes, net, optimizer)\n",
        "#         vs_random_once = vs_random(net)\n",
        "#         print('vs_random = ', sorted(vs_random_once.items()), end='')\n",
        "#         for r, n in vs_random_once.items():\n",
        "#             vs_random_sum[r] += n\n",
        "#         print(' sum = ', sorted(vs_random_sum.items()))\n",
        "#         #show_net(net, State())\n",
        "#         #show_net(net, State().play('A1 C1 A2 C2'))\n",
        "#         #show_net(net, State().play('A1 B2 C3 B3 C1'))\n",
        "#         #show_net(net, State().play('B2 A2 A3 C1 B3'))\n",
        "#         #show_net(net, State().play('B2 A2 A3 C1'))\n",
        "# print('finished')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syV9LTxE7_f3"
      },
      "outputs": [],
      "source": [
        "net = Net().to('cuda')\n",
        "net.load_state_dict(torch.load('network.pkl'))\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVb51QoNeYjz"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d1b4IrSp6GJ",
        "outputId": "b261ba57-365c-42ca-b7ad-82c9a3192c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "initial state\n",
            " 1 2 3 4 5 6 7\n",
            "A . . . . . . . \n",
            "B  . . . . . . . \n",
            "C   . . . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n",
            "p = \n",
            "[[[28 39 17 17 18 18 19]\n",
            "  [19 19 18 18 18 19 20]\n",
            "  [20 20 18 20 20 19 19]\n",
            "  [20 20 18 19 19 19 19]\n",
            "  [20 19 19 20 20 18 19]\n",
            "  [31 19 19 20 21 20 19]\n",
            "  [19 18 20 19 19 19 19]]]\n",
            "v =  -0.65052974\n",
            "\n",
            "WIN by put\n",
            " 1 2 3 4 5 6 7\n",
            "A y y . . . . . \n",
            "B  . . . . . . . \n",
            "C   p p . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n",
            "p = \n",
            "[[[  0   0   2   0   0   0   0]\n",
            "  [ 47 226   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]\n",
            "  [  2 719   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]]]\n",
            "v =  -0.73141724\n",
            "\n",
            "LOSE by opponent's double\n",
            " 1 2 3 4 5 6 7\n",
            "A . o y . . . . \n",
            "B  . y y . . . . \n",
            "C   p . . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n",
            "p = \n",
            "[[[  0   0   0   0   0   0   0]\n",
            "  [976   0   0   0   0   0   0]\n",
            "  [  0  10   0   1   0   0   0]\n",
            "  [ 10   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]]]\n",
            "v =  -0.5826329\n",
            "\n",
            "WIN through double\n",
            " 1 2 3 4 5 6 7\n",
            "A . o y . . . . \n",
            "B  . y . . . . . \n",
            "C   p . . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n",
            "p = \n",
            "[[[  0   0   0   0   0   0   0]\n",
            "  [299   0   0   0   0   0   0]\n",
            "  [  0   1   0   0   0   0   0]\n",
            "  [697   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]\n",
            "  [  0   0   0   0   0   0   0]]]\n",
            "v =  -0.7117713\n",
            "\n",
            "strategic WIN by following double\n",
            " 1 2 3 4 5 6 7\n",
            "A . . o . . . . \n",
            "B  x . . . . . . \n",
            "C   . . . . . . . \n",
            "D    . . . . . . . \n",
            "E     . . . . . . . \n",
            "F      . . . . . . . \n",
            "G       . . . . . . . \n",
            "p = \n",
            "[[[858  20   0   1   1   1   1]\n",
            "  [  0  10   1   1   1   1   1]\n",
            "  [  3  15   1   1   1   1   1]\n",
            "  [  4  10   1   1   1   1   1]\n",
            "  [  8   2   1   1   1   1   1]\n",
            "  [  3   1   1   1   1   1   1]\n",
            "  [  1   1   1   1   1   1   1]]]\n",
            "v =  -0.64633363\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show outputs from trained net\n",
        "\n",
        "print('initial state')\n",
        "show_net(net, State())\n",
        "\n",
        "print('WIN by put')\n",
        "show_net(net, State().play('A1 C1 A2 C2'))\n",
        "\n",
        "print('LOSE by opponent\\'s double')\n",
        "show_net(net, State().play('B2 A2 A3 C1 B3'))\n",
        "\n",
        "print('WIN through double')\n",
        "show_net(net, State().play('B2 A2 A3 C1'))\n",
        "\n",
        "# hard case: putting on A1 will cause double\n",
        "print('strategic WIN by following double')\n",
        "show_net(net, State().play('B1 A3'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMq1AhTW4VHj"
      },
      "source": [
        "# Play Against Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L3pDjn0AK_p",
        "outputId": "15139013-00d9-4773-ac63-304f723173f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hexalattice\n",
            "  Downloading hexalattice-1.2.1.tar.gz (8.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from hexalattice) (1.21.6)\n",
            "Requirement already satisfied: matplotlib>=3.1 in /usr/local/lib/python3.7/dist-packages (from hexalattice) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->hexalattice) (3.0.8)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->hexalattice) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->hexalattice) (1.4.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.1->hexalattice) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.1->hexalattice) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.1->hexalattice) (1.15.0)\n",
            "Building wheels for collected packages: hexalattice\n",
            "  Building wheel for hexalattice (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hexalattice: filename=hexalattice-1.2.1-py3-none-any.whl size=7674 sha256=50f07ff815c7308edc9a6dfbcd425cb06dd6be4200b5336067897a4625c06565\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/0b/e5/a95d7742854afc499500077663c527fc212e03c4117ee1031f\n",
            "Successfully built hexalattice\n",
            "Installing collected packages: hexalattice\n",
            "Successfully installed hexalattice-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install hexalattice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXV3BfWEA76T"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_heatmap(A):\n",
        "  X, Y = np.meshgrid(range(A.shape[0]), range(A.shape[-1]))\n",
        "  X, Y = X*2, Y*2\n",
        "  \n",
        "  # Turn this into a hexagonal grid\n",
        "  for i, k in enumerate(X):\n",
        "      if i % 2 == 1:\n",
        "          X[i] += 1\n",
        "          Y[:,i] += 1\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.hexbin(\n",
        "      X.reshape(-1), \n",
        "      Y.reshape(-1), \n",
        "      C=A.reshape(-1), \n",
        "      gridsize=int(A.shape[0]/2)\n",
        "  )\n",
        "\n",
        "  # the rest of the code is adjustable for best output\n",
        "  ax.set_aspect(0.8)\n",
        "  ax.set(xlim=(-4, X.max()+4,), ylim=(-4, Y.max()+4))\n",
        "  ax.axis(False)\n",
        "  plt.colorbar(im)\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w2LWyl8p6GK"
      },
      "outputs": [],
      "source": [
        "# Search with trained net\n",
        "\n",
        "tree = Tree(net)\n",
        "state = State()\n",
        "while True:\n",
        "  user_input = input(\"Input move: \")\n",
        "  state.play(user_input)\n",
        "  if state.terminal():\n",
        "    break\n",
        "  distb = tree.think(state, 500, temperature=1, show=True)\n",
        "  pv_seq = tree.pv(state)\n",
        "  state.play(pv_seq[0])\n",
        "  display_heatmap(distb.reshape((7,7)))\n",
        "  print(state)\n",
        "  if state.terminal():\n",
        "    break\n",
        "print(state)\n",
        "print(state.terminal_reward())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nmT85hx4TdB"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "BotsForGames_Sprint2.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
