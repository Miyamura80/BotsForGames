{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miyamura80/BotsForGames/blob/main/BotsForGames_Sprint2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0DCN2-Op6F2",
        "outputId": "b7a1b2cc-81cc-4981-aa26-2b478a1e421d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Collecting open_spiel\n",
            "  Downloading open_spiel-1.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (21.4.0)\n",
            "Requirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (21.1.3)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (1.21.6)\n",
            "Collecting scipy>=1.5.4\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.10.0->open_spiel) (1.15.0)\n",
            "Installing collected packages: scipy, open-spiel\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed open-spiel-1.1.0 scipy-1.7.3\n"
          ]
        }
      ],
      "source": [
        "# environment:\n",
        "!pip3 install torch\n",
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prmwuNkmWU5g"
      },
      "source": [
        "# Implementation of hex board state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ycSlk7q8p6F4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pyspiel\n",
        "import copy\n",
        "\n",
        "BOARD_SIZE = 5\n",
        "game = pyspiel.load_game(\"hex\",{\"board_size\":BOARD_SIZE})\n",
        "BLACK, WHITE = 1, -1  # first turn or second turn player\n",
        "\n",
        "class State:\n",
        "    '''Board implementation of BOARD_SIZE x BOARD_SIZE Hex Board'''\n",
        "    X, Y = 'ABCDEFGHI'[0:BOARD_SIZE],  '123456789'[0:BOARD_SIZE]\n",
        "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE)) # (x, y)\n",
        "        self.color = 1\n",
        "        self.win_color = 0\n",
        "        self.record = []\n",
        "        self.hex_state = game.new_initial_state()\n",
        "\n",
        "    def action2str(self, a: int):\n",
        "        return self.X[a // BOARD_SIZE] + self.Y[a % BOARD_SIZE]\n",
        "\n",
        "    def str2action(self, s: str):\n",
        "        return self.X.find(s[0]) * BOARD_SIZE + self.Y.find(s[1])\n",
        "\n",
        "    def record_string(self):\n",
        "        return ' '.join([self.action2str(a) for a in self.record])\n",
        "    \n",
        "    def __deepcopy__(self):\n",
        "        newState = State()\n",
        "        newState.board = copy.deepcopy(self.board)\n",
        "        newState.win_color = copy.deepcopy(self.win_color)\n",
        "        newState.record = copy.deepcopy(self.record)\n",
        "        newState.hex_state = copy.deepcopy(self.hex_state)\n",
        "        return newState\n",
        "\n",
        "    def __str__(self):\n",
        "        final_bd = [\" \"+\" \".join(self.Y)]\n",
        "        hex_bd = str(self.hex_state).split(\"\\n\")\n",
        "        for i in range(len(hex_bd)):\n",
        "            final_bd.append(self.X[i]+\" \"+hex_bd[i])\n",
        "        return \"\\n\".join(final_bd)\n",
        "\n",
        "    def play(self, action):\n",
        "        # state transition function\n",
        "        # action is position interger (0~8) or string representation of action sequence\n",
        "        # Handles the case where action is sequence of actions \"0 1 2 3 4\"\n",
        "        if isinstance(action, str):\n",
        "            for astr in action.split():\n",
        "                self.play(self.str2action(astr))\n",
        "            return self\n",
        "\n",
        "        # Single action case\n",
        "        x, y = action // BOARD_SIZE, action % BOARD_SIZE\n",
        "        self.board[x, y] = self.color\n",
        "        self.hex_state.apply_action(action)\n",
        "\n",
        "        # check whether 3 stones are on the line\n",
        "        if self.hex_state.is_terminal():\n",
        "            self.win_color = self.color\n",
        "\n",
        "        self.color = -self.color\n",
        "        self.record.append(action)\n",
        "        return self\n",
        "\n",
        "    def terminal(self):\n",
        "        # terminal state check\n",
        "        return self.hex_state.is_terminal()\n",
        "\n",
        "    def terminal_reward(self):\n",
        "        # terminal reward \n",
        "        # return self.win_color if self.color == BLACK else -self.win_color\n",
        "        return self.win_color\n",
        "\n",
        "    def legal_actions(self):\n",
        "        # list of legal actions on each state\n",
        "        return [a for a in range(BOARD_SIZE * BOARD_SIZE) if self.board[a // BOARD_SIZE, a % BOARD_SIZE] == 0]\n",
        "\n",
        "    def feature(self):\n",
        "        # input tensor for neural net (state)\n",
        "        # return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
        "        observation =  np.array(self.hex_state.observation_tensor(), np.float32)\n",
        "        return observation.reshape(9,BOARD_SIZE,BOARD_SIZE)\n",
        "\n",
        "    def action_feature(self, action):\n",
        "        # input tensor for neural net (action)\n",
        "        a = np.zeros((1, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n",
        "        a[0, action // BOARD_SIZE, action % BOARD_SIZE] = 1\n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhwWeH5GWc5N"
      },
      "source": [
        "# Small neural nets with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4TlYnoxp6GA",
        "outputId": "9e92d2d6-21a3-4870-cdfb-d0691f39c48b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Small neural nets with PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "# convnet from input channels w number filters0, output channels w number filters1\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
        "        self.bn = None\n",
        "        if bn:\n",
        "            self.bn = nn.BatchNorm2d(filters1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            h = self.bn(h)\n",
        "        return h\n",
        "\n",
        "# 3x3 conv filter with same input & output channels \n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, filters):\n",
        "        super().__init__()\n",
        "        self.conv = Conv(filters, filters, 3, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu(x + (self.conv(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhPImlQ1WhOD"
      },
      "source": [
        "# Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KUYX23urp6GC"
      },
      "outputs": [],
      "source": [
        "num_filters = 16\n",
        "num_blocks = 4\n",
        "\n",
        "class Representation(nn.Module):\n",
        "    ''' Conversion from observation to inner abstract state '''\n",
        "    def __init__(self, input_shape):\n",
        "        super().__init__()\n",
        "        self.input_shape = input_shape\n",
        "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
        "\n",
        "        self.layer0 = Conv(self.input_shape[0], num_filters, BOARD_SIZE, bn=True)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = F.relu(self.layer0(x))\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "    def inference(self, x):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            rp = self(torch.from_numpy(x).unsqueeze(0).to('cuda'))\n",
        "        return rp.cpu().numpy()[0]\n",
        "\n",
        "class Prediction(nn.Module):\n",
        "    ''' Policy and value prediction from inner abstract state '''\n",
        "    def __init__(self, action_shape):\n",
        "        super().__init__()\n",
        "        self.board_size = np.prod(action_shape[1:])\n",
        "        self.action_size = action_shape[0] * self.board_size\n",
        "\n",
        "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.conv_p2 = Conv(4, 1, 1)\n",
        "\n",
        "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
        "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
        "\n",
        "    def forward(self, rp):\n",
        "        h_p = F.relu(self.conv_p1(rp))\n",
        "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
        "\n",
        "        h_v = F.relu(self.conv_v(rp))\n",
        "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
        "\n",
        "        # range of value is -1 ~ 1\n",
        "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
        "\n",
        "    def inference(self, rp):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            p, v = self(torch.from_numpy(rp).unsqueeze(0).to('cuda'))\n",
        "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]\n",
        "\n",
        "class Dynamics(nn.Module):\n",
        "    '''Abstract state transition'''\n",
        "    def __init__(self, rp_shape, act_shape):\n",
        "        super().__init__()\n",
        "        self.rp_shape = rp_shape\n",
        "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
        "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, rp, a):\n",
        "        h = torch.cat([rp, a], dim=1)\n",
        "        h = self.layer0(h)\n",
        "        for block in self.blocks:\n",
        "            h = block(h)\n",
        "        return h\n",
        "\n",
        "    def inference(self, rp, a):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            rp = self(torch.from_numpy(rp).unsqueeze(0).to('cuda'), torch.from_numpy(a).unsqueeze(0).to('cuda'))\n",
        "        return rp.cpu().numpy()[0]\n",
        "\n",
        "''' Collects all h, f, g functions in one net, and predict outcome ''' \n",
        "class Net(nn.Module):\n",
        "    '''Whole net'''\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        state = State()\n",
        "        input_shape = state.feature().shape\n",
        "        print(input_shape)\n",
        "        action_shape = state.action_feature(0).shape\n",
        "        rp_shape = (num_filters, *input_shape[1:])\n",
        "\n",
        "        self.representation = Representation(input_shape)\n",
        "        self.prediction = Prediction(action_shape)\n",
        "        self.dynamics = Dynamics(rp_shape, action_shape)\n",
        "\n",
        "    def predict(self, state0, path):\n",
        "        '''\n",
        "        Predict p and v from original state and path\n",
        "        Return [(p1,v1), (p2,v2), ...]\n",
        "        Where path :: [Action]\n",
        "        '''\n",
        "        \n",
        "        outputs = []\n",
        "        x = state0.feature()\n",
        "        rp = self.representation.inference(x)\n",
        "        outputs.append(self.prediction.inference(rp))\n",
        "        for action in path:\n",
        "            a = state0.action_feature(action)\n",
        "            rp = self.dynamics.inference(rp, a)\n",
        "            outputs.append(self.prediction.inference(rp))\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nq-MFjop6GE",
        "outputId": "f2014704-3835-422f-f342-3b67ecb3570a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 5, 5)\n",
            " 1 2 3 4 5\n",
            "A . . . . . \n",
            "B  . . . . . \n",
            "C   . . . . . \n",
            "D    . . . . . \n",
            "E     . . . . . \n",
            "p = \n",
            "[[[38 39 39 41 41]\n",
            "  [39 40 39 41 40]\n",
            "  [39 40 38 40 40]\n",
            "  [40 41 38 39 39]\n",
            "  [40 41 39 39 37]]]\n",
            "v =  0.005403269\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def show_net(net, state):\n",
        "    '''Display policy (p) and value (v)'''\n",
        "    print(state)\n",
        "    p, v = net.predict(state, [])[-1]\n",
        "    print('p = ')\n",
        "    print((p * 1000).astype(int).reshape((-1, *net.representation.input_shape[1:3])))\n",
        "    print('v = ', v)\n",
        "    print()\n",
        "\n",
        "#  Outputs before training\n",
        "show_net(Net().to('cuda'), State())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Heatmap"
      ],
      "metadata": {
        "id": "BZTCVDVNDWYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_heatmap2(grid):\n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.imshow(grid)\n",
        "  # Loop over data dimensions and create text annotations.\n",
        "  for i in range(grid.shape[0]):\n",
        "      for j in range(grid.shape[1]):\n",
        "          text = ax.text(j, i, round(grid[i, j],2),\n",
        "                        ha=\"center\", va=\"center\", color=\"w\")\n",
        "  plt.show()\n",
        "\n",
        "def display_heatmap(A):\n",
        "  X, Y = np.meshgrid(range(A.shape[0]), range(A.shape[-1]))\n",
        "  X, Y = X*2, Y*2\n",
        "  \n",
        "  # Turn this into a hexagonal grid\n",
        "  for i, k in enumerate(X):\n",
        "      if i % 2 == 1:\n",
        "          X[i] += 1\n",
        "          Y[:,i] += 1\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  im = ax.hexbin(\n",
        "      X.reshape(-1), \n",
        "      Y.reshape(-1), \n",
        "      C=A.reshape(-1), \n",
        "      gridsize=int(A.shape[0]/2)\n",
        "  )\n",
        "\n",
        "  # the rest of the code is adjustable for best output\n",
        "  ax.set_aspect(0.8)\n",
        "  ax.set(xlim=(-4, X.max()+4,), ylim=(-4, Y.max()+4))\n",
        "  ax.axis(False)\n",
        "  plt.colorbar(im)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "nA3-yLSjDX8m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekC6XGI4WQ--"
      },
      "source": [
        "# MuZero Monte Carlo Tree Search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rqlkZ8MCp6GE"
      },
      "outputs": [],
      "source": [
        "# Implementation of Monte Carlo Tree Search\n",
        "\n",
        "class Node:\n",
        "    '''Search result of one abstract (or root) state'''\n",
        "    def __init__(self, p, v):#\n",
        "        ''' Policy and Value '''\n",
        "        self.p, self.v = p, v\n",
        "        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)\n",
        "        self.n_all, self.q_sum_all = 1, v / 2 # prior\n",
        "\n",
        "    def update(self, action, q_new):\n",
        "        # Update\n",
        "        self.n[action] += 1\n",
        "        self.q_sum[action] += q_new\n",
        "\n",
        "        # Update overall stats\n",
        "        self.n_all += 1\n",
        "        self.q_sum_all += q_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nMuA1Ednp6GF"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "class Tree:\n",
        "    '''Monte Carlo Tree'''\n",
        "    def __init__(self, net):\n",
        "        self.net = net\n",
        "        self.nodes = {}\n",
        "\n",
        "    def search(self, state, path, rp, depth):\n",
        "        # Return predicted value from new state\n",
        "        key = state.record_string()\n",
        "        if len(path) > 0:\n",
        "            key += '|' + ' '.join(map(state.action2str, path))\n",
        "        if key not in self.nodes:\n",
        "            p, v = self.net.prediction.inference(rp)\n",
        "            self.nodes[key] = Node(p, v)\n",
        "            return v\n",
        "\n",
        "        # State transition by an action selected from bandit\n",
        "        node = self.nodes[key]\n",
        "        p = node.p\n",
        "        mask = np.zeros_like(p)\n",
        "        if depth == 0:\n",
        "            # Add noise to policy on the root node\n",
        "            p = 0.75 * p + 0.25 * np.random.dirichlet([0.15] * len(p))\n",
        "            # On the root node, we choose action only from legal actions\n",
        "            mask[state.legal_actions()] = 1\n",
        "            p *= mask\n",
        "            p /= p.sum() + 1e-16\n",
        "\n",
        "        n, q_sum = 1 + node.n, node.q_sum_all / node.n_all + node.q_sum\n",
        "        ucb = q_sum / n + 2.0 * np.sqrt(node.n_all) * p / n + mask * 4 # PUCB formula\n",
        "        best_action = np.argmax(ucb)\n",
        "\n",
        "        # Search next state by recursively calling this function\n",
        "        rp_next = self.net.dynamics.inference(rp, state.action_feature(best_action))\n",
        "        path.append(best_action)\n",
        "        q_new = -self.search(state, path, rp_next, depth + 1) # With the assumption of changing player by turn\n",
        "        node.update(best_action, q_new)\n",
        "\n",
        "        return q_new\n",
        "\n",
        "    def think(self, state, num_simulations, temperature = 0, show=False):\n",
        "        # End point of MCTS\n",
        "        if show:\n",
        "            print(state)\n",
        "        start, prev_time = time.time(), 0\n",
        "        for _ in range(num_simulations):\n",
        "            self.search(state, [], self.net.representation.inference(state.feature()), depth=0)\n",
        "\n",
        "            # Display search result on every second\n",
        "            if show:\n",
        "                tmp_time = time.time() - start\n",
        "                if int(tmp_time) > int(prev_time):\n",
        "                    prev_time = tmp_time\n",
        "                    root, pv = self.nodes[state.record_string()], self.pv(state)\n",
        "                    n = root.n + 1\n",
        "                    n = (n / np.max(n)) ** (1 / (temperature + 1e-8))\n",
        "                    disp_grid = (n / n.sum()).reshape(BOARD_SIZE, BOARD_SIZE)\n",
        "                    display_heatmap2(disp_grid)\n",
        "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'\n",
        "                          % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],\n",
        "                             root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))\n",
        "\n",
        "        #  Return probability distribution weighted by the number of simulations\n",
        "        root = self.nodes[state.record_string()]\n",
        "        n = root.n + 1\n",
        "        n = (n / np.max(n)) ** (1 / (temperature + 1e-8))\n",
        "        return n / n.sum()\n",
        "\n",
        "    def pv(self, state):\n",
        "        # Return principal variation (action sequence which is considered as the best)\n",
        "        s, pv_seq = state.__deepcopy__(), []\n",
        "        while True:\n",
        "            key = s.record_string()\n",
        "            if key not in self.nodes or self.nodes[key].n.sum() == 0:\n",
        "                break\n",
        "            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]\n",
        "            pv_seq.append(best_action)\n",
        "            s.play(best_action)\n",
        "        return pv_seq\n",
        "    \n",
        "    def values(self, state):\n",
        "        s, pv_seq = copy.deepcopy(state), []\n",
        "        key = s.record_string()\n",
        "        if key not in self.nodes:\n",
        "            return np.zeros(BOARD_SIZE*BOARD_SIZE).reshape((BOARD_SIZE,BOARD_SIZE))\n",
        "\n",
        "        result = np.zeros(BOARD_SIZE*BOARD_SIZE)\n",
        "        best_actions = [(a, self.nodes[key].n[a]) for a in s.legal_actions()]\n",
        "        for (a, v) in best_actions:\n",
        "            result[a] = v\n",
        "        return result.reshape((BOARD_SIZE,BOARD_SIZE))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGrCluKrp6GG",
        "outputId": "35121e9c-44e1-4525-db80-ccfce43b141b",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 5, 5)\n",
            " 1 2 3 4 5\n",
            "A . . . . . \n",
            "B  . . . . . \n",
            "C   . . . . . \n",
            "D    . . . . . \n",
            "E     . . . . . \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4., 5., 3., 3., 4.],\n",
              "       [3., 3., 3., 4., 3.],\n",
              "       [3., 5., 4., 3., 4.],\n",
              "       [4., 4., 5., 5., 4.],\n",
              "       [4., 5., 5., 5., 4.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Search with initialized net\n",
        "\n",
        "tree = Tree(Net().to('cuda'))\n",
        "\n",
        "tree.think(State(), 100, show=True)\n",
        "tree.values(State())\n",
        "\n",
        "# tree = Tree(Net().to('cuda'))\n",
        "# tree.think(State().play('A1 C1 A2 C2'), 200, show=True)\n",
        "\n",
        "# tree = Tree(Net().to('cuda'))\n",
        "# tree.think(State().play('B2 A2 A3 C1 B3'), 200, show=True)\n",
        "\n",
        "# tree = Tree(Net().to('cuda'))\n",
        "# tree.think(State().play('B2 A2 A3 C1'), 200, show=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0anzRbZYV718"
      },
      "source": [
        "# Training of neural net\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "hGJtw40qp6GH"
      },
      "outputs": [],
      "source": [
        "# Training of neural net\n",
        "\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "final_ploss = []\n",
        "final_vloss = []\n",
        "\n",
        "batch_size = 32\n",
        "num_steps = 100\n",
        "\n",
        "def gen_target(ep, k):\n",
        "    '''Generate inputs and targets for training'''\n",
        "    # path, reward, observation, action, policy\n",
        "    turn_idx = np.random.randint(len(ep[0]))\n",
        "    ps, vs, ax = [], [], []\n",
        "    for t in range(turn_idx, turn_idx + k + 1):\n",
        "        if t < len(ep[0]):\n",
        "            p = ep[4][t]\n",
        "            a = ep[3][t]\n",
        "        else: # state after finishing game\n",
        "            # p is 0 (loss is 0)\n",
        "            p = np.zeros_like(ep[4][-1])\n",
        "            # random action selection\n",
        "            a = np.zeros(np.prod(ep[3][-1].shape), dtype=np.float32)\n",
        "            a[np.random.randint(len(a))] = 1\n",
        "            a = a.reshape(ep[3][-1].shape)\n",
        "        vs.append([ep[1] if t % 2 == 0 else -ep[1]])\n",
        "        ps.append(p)\n",
        "        ax.append(a)\n",
        "        \n",
        "    return ep[2][turn_idx], ax, ps, vs\n",
        "\n",
        "def train(episodes, net, opt):\n",
        "    '''Train neural net'''\n",
        "    p_loss_sum, v_loss_sum = 0, 0\n",
        "    net.train()\n",
        "    k = 4\n",
        "    for _ in range(num_steps):\n",
        "        x, ax, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))], k) for j in range(batch_size)])\n",
        "        x = torch.from_numpy(np.array(x)).to('cuda')\n",
        "        ax = torch.from_numpy(np.array(ax)).to('cuda')\n",
        "        p_target = torch.from_numpy(np.array(p_target)).to('cuda')\n",
        "        v_target = torch.FloatTensor(np.array(v_target)).to('cuda')\n",
        "\n",
        "        # Change the order of axis as [time step, batch, ...]\n",
        "        ax = torch.transpose(ax, 0, 1)\n",
        "        p_target = torch.transpose(p_target, 0, 1)\n",
        "        v_target = torch.transpose(v_target, 0, 1)\n",
        "\n",
        "        # Compute losses for k (+ current) steps\n",
        "        p_loss, v_loss = 0, 0\n",
        "        for t in range(k + 1):\n",
        "            rp = net.representation(x) if t == 0 else net.dynamics(rp, ax[t - 1])\n",
        "            p, v = net.prediction(rp)\n",
        "            p_loss += F.kl_div(torch.log(p), p_target[t], reduction='sum')\n",
        "            v_loss += torch.sum(((v_target[t] - v) ** 2) / 2)\n",
        "\n",
        "        p_loss_sum += p_loss.item()\n",
        "        v_loss_sum += v_loss.item()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = Variable(p_loss+v_loss, requires_grad=True)\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    num_train_datum = num_steps * batch_size\n",
        "    global final_ploss, final_vloss \n",
        "    final_ploss.append(p_loss_sum / num_train_datum)\n",
        "    final_vloss.append(v_loss_sum / num_train_datum)\n",
        "    print('p_loss %f v_loss %f' % (p_loss_sum / num_train_datum, v_loss_sum / num_train_datum))\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dhDo47mV_dj"
      },
      "source": [
        "#  Battle against random agents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kRo8A-WTp6GI"
      },
      "outputs": [],
      "source": [
        "#  Battle against random agents\n",
        "\n",
        "def vs_random(net, n=100):\n",
        "    results = {}\n",
        "    for i in range(n):\n",
        "        first_turn = i % 2 == 0\n",
        "        turn = first_turn\n",
        "        state = State()\n",
        "        while not state.terminal():\n",
        "            if turn:\n",
        "                p, _ = net.predict(state, [])[-1]\n",
        "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\n",
        "            else:\n",
        "                action = np.random.choice(state.legal_actions())\n",
        "            state.play(action)\n",
        "            turn = not turn\n",
        "        r = state.terminal_reward() if turn else -state.terminal_reward()\n",
        "        results[r] = results.get(r, 0) + 1\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MCTS Agent"
      ],
      "metadata": {
        "id": "0s1VnSrthXm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import random\n",
        "import time\n",
        "from math import sqrt, log\n",
        "from collections import defaultdict\n",
        "\n",
        "random.seed(1)\n",
        "class MCTSAgent:\n",
        "    def __init__(self) -> None:\n",
        "        self.best = []\n",
        "        # Both of these :: path -> dict[move, x]\n",
        "        self.moves = defaultdict(lambda: defaultdict(int))\n",
        "        self.reward = defaultdict(lambda: defaultdict(float))\n",
        "    \n",
        "    def ucb_weight_general(self, state, mv, epoch, c=2.0):\n",
        "        path = state.record_string()\n",
        "        expected_reward = self.reward[path][mv]/(self.moves[path][mv]+1)\n",
        "        n_visit = self.moves[path][mv]\n",
        "        return expected_reward + c * sqrt(log(epoch)/(n_visit+1))\n",
        "\n",
        "    def think(self, state: State, sim_num: int, temperature:int, show=False) -> None:\n",
        "        if show:\n",
        "            print(\"Bot to play: \\n\", state, state.color)\n",
        "\n",
        "        start, prev_time = time.time(), 0        \n",
        "        if state.terminal():\n",
        "            return\n",
        "        \n",
        "        init_path = state.record_string()\n",
        "\n",
        "        for epoch in range(1, sim_num):\n",
        "            freshState = state.__deepcopy__()\n",
        "            # Display search result on every second\n",
        "            if show:\n",
        "                tmp_time = time.time() - start\n",
        "                if int(tmp_time) > int(prev_time):\n",
        "                    prev_time = tmp_time\n",
        "                    pv = self.pv(freshState)\n",
        "                    total_rew = sum([v for k,v in self.reward[init_path].items()])\n",
        "                    display_moves = [[self.reward[init_path][i*BOARD_SIZE + j]/total_rew if i*BOARD_SIZE+j in state.legal_actions() else 0 \\\n",
        "                                      for j in range(BOARD_SIZE)] for i in range(BOARD_SIZE)]\n",
        "                    display_heatmap2(np.array(display_moves))\n",
        "                    # print('%.2f sec. best %s. q = %.4f. n = %d / %d.'\n",
        "                    #       % (tmp_time, state.action2str(pv[0]), self.reward[init_path][pv[0]] / (self.moves[init_path][pv[0]]+1),\n",
        "                    #          self.moves[pv[0]], epoch))\n",
        "            not_terminated = True\n",
        "            rewards = []\n",
        "            while not_terminated:\n",
        "                # first_move = random.choice(list(self.moves))\n",
        "                path = freshState.record_string()\n",
        "                ucb_weights = [self.ucb_weight_general(freshState, k, epoch) for k in freshState.legal_actions()]\n",
        "                max_ucb_weight = max(ucb_weights)\n",
        "                move = [k for k in freshState.legal_actions() if self.ucb_weight_general(freshState, k, epoch)==max_ucb_weight][0]\n",
        "                if move in self.moves[path]:\n",
        "                  self.moves[path][move] += 1\n",
        "                else:\n",
        "                  self.moves[path][move] = 1\n",
        "                freshState.play(move)\n",
        "                if path not in self.reward:\n",
        "                  self.reward[path] = {move: 0}\n",
        "                rewards.append((self.reward[path], move))  \n",
        "                not_terminated = not freshState.terminal()\n",
        "            for (r,m) in rewards:\n",
        "                r[m] += freshState.terminal_reward()\n",
        "        init_rewards = self.reward[init_path]\n",
        "        total_rew = sum([v for k,v in init_rewards.items()])\n",
        "        move_policy = [init_rewards[i]/total_rew if i in state.legal_actions() else 0 for i in range(BOARD_SIZE*BOARD_SIZE)]\n",
        "        return move_policy\n",
        "\n",
        "    def pv(self, state: State) -> List[int]:\n",
        "        path = state.record_string()\n",
        "        if path in self.reward:\n",
        "          max_value = max(self.reward[path].values())\n",
        "          max_moves = [k for k,v in self.reward[path].items() if v==max_value]\n",
        "          print(f\"Max Value: {max_value} Rewards: {self.reward[path]} Moves: {self.moves[path]}\")\n",
        "        else:\n",
        "          max_moves = state.legal_actions()\n",
        "          # print(\"ah\")\n",
        "        return [random.choice(max_moves)]\n"
      ],
      "metadata": {
        "id": "J-Rq2s8ihTUi"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Benchmarking\n"
      ],
      "metadata": {
        "id": "LgLkjXTJg9f8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from random import choice\n",
        "\n",
        "DEFAULT_ELO = 1000\n",
        "CONST_K = 32\n",
        "\n",
        "# Return elo\n",
        "def benchmark(tree, eval_num=15, net_think_time=500, mcts_think_time=500, show=False) -> float:\n",
        "  mcts_agent = MCTSAgent()\n",
        "  elos = {0: DEFAULT_ELO, 1: DEFAULT_ELO}\n",
        "  # 0 means net goes first, 1 means net goes second\n",
        "  net_play_order = random.choice([0,1])\n",
        "\n",
        "  get_think_time = lambda x: net_think_time if x==net_play_order else mcts_think_time\n",
        "  agent1 = tree if net_play_order==0 else mcts_agent\n",
        "  agent2 = tree if net_play_order==1 else mcts_agent\n",
        "\n",
        "  for eval in range(eval_num):\n",
        "    print(net_play_order, \"is net. Elos\", elos)\n",
        "    game_on = True\n",
        "    state = State()\n",
        "    while game_on:\n",
        "  \n",
        "      distb = agent1.think(state, get_think_time(0), temperature=1)\n",
        "      pv_seq = agent1.pv(state)\n",
        "      state.play(pv_seq[0])\n",
        "\n",
        "      if show:\n",
        "        print(state, state.color)\n",
        "\n",
        "      if state.terminal():\n",
        "        game_on = False\n",
        "      else:      \n",
        "        distb = agent2.think(state, get_think_time(1), temperature=1)\n",
        "        pv_seq = agent2.pv(state)\n",
        "        state.play(pv_seq[0])\n",
        "        game_on = not state.terminal()\n",
        "    final_reward = state.terminal_reward()\n",
        "    winner_index = 0 if final_reward==1 else 1\n",
        "    score_func = lambda x,y: 1 if x==y else 0\n",
        "    for index in [0,1]:\n",
        "      elos[index] = elos[index] \\\n",
        "       + CONST_K*(score_func(index, winner_index) - 1/(1+10**((elos[1-index] - elos[index])/400)))\n",
        "  return elos[net_play_order]\n",
        "  "
      ],
      "metadata": {
        "id": "WxoTL32zg-5k"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XztEthigVxVT"
      },
      "source": [
        "# Main Algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XviavCidp6GJ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# # Main algorithm of MuZero\n",
        "\n",
        "# num_games = 10000\n",
        "# num_games_one_epoch = 20\n",
        "# num_simulations = 40\n",
        "\n",
        "# net = Net()\n",
        "\n",
        "# net.to('cuda')\n",
        "# optimizer = optim.SGD(net.parameters(), lr=3e-4, weight_decay=3e-5, momentum=0.8)\n",
        "\n",
        "# # Display battle results as {-1: lose 0: draw 1: win} (for episode generated for training, 1 means that the first player won)\n",
        "# vs_random_sum = vs_random(net)\n",
        "# print('vs_random = ', sorted(vs_random_sum.items()))\n",
        "\n",
        "# episodes = []\n",
        "# result_distribution = {1: 0, 0: 0, -1: 0}\n",
        "\n",
        "# for g in range(num_games):\n",
        "#     # Generate one episode\n",
        "#     record, p_targets, features, action_features = [], [], [], []\n",
        "#     state = State()\n",
        "#     # temperature using to make policy targets from search results\n",
        "#     temperature = 0.7\n",
        "\n",
        "#     # Carry out single game of MCTS representation search (with real actions)\n",
        "#     while not state.terminal():\n",
        "#         tree = Tree(net)\n",
        "#         # Predicted policy after MCTS simulations\n",
        "#         p_target = tree.think(state, num_simulations, temperature)\n",
        "\n",
        "#         # F.R.\n",
        "#         p_targets.append(p_target)\n",
        "#         features.append(state.feature())\n",
        "\n",
        "#         # Only choose legal actions, normalize predicted policy\n",
        "#         p_target = [p_target[i] if i in state.legal_actions() else 0 for i in range(len(p_target))]\n",
        "#         p_target = np.array(p_target) / sum(p_target)\n",
        "\n",
        "#         # Choose action\n",
        "#         action = np.random.choice(np.arange(len(p_target)), p=p_target)\n",
        "\n",
        "#         # F.R.\n",
        "#         record.append(action)\n",
        "#         action_features.append(state.action_feature(action))\n",
        "\n",
        "#         state.play(action)\n",
        "#         temperature *= 0.8\n",
        "\n",
        "#     # reward seen from the first turn player\n",
        "#     reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\n",
        "#     result_distribution[reward] += 1\n",
        "#     episodes.append((record, reward, features, action_features, p_targets))\n",
        "\n",
        "#     if g % num_games_one_epoch == 0:\n",
        "#         print('game ', end='')\n",
        "#     print(g, ' ', end='')\n",
        "\n",
        "#     # Training of neural net\n",
        "#     if (g + 1) % num_games_one_epoch == 0:\n",
        "#         # Show the result distributiuon of generated episodes\n",
        "#         print('generated = ', sorted(result_distribution.items()))\n",
        "#         net = train(episodes, net, optimizer)\n",
        "#         vs_random_once = vs_random(net)\n",
        "#         print('vs_random = ', sorted(vs_random_once.items()), end='')\n",
        "#         for r, n in vs_random_once.items():\n",
        "#             vs_random_sum[r] += n\n",
        "#         print(' sum = ', sorted(vs_random_sum.items()))\n",
        "\n",
        "# print('finished')\n",
        "\n",
        "# try:\n",
        "#     fig, ax1 = plt.subplots()\n",
        "\n",
        "#     color = 'tab:red'\n",
        "#     ax1.set_xlabel('sample epoch')\n",
        "#     ax1.set_ylabel('p_loss', color=color)\n",
        "#     ax1.plot(final_ploss, color=color)\n",
        "#     ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "#     ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "\n",
        "#     color = 'tab:blue'\n",
        "#     ax2.set_ylabel('v_loss', color=color)  # we already handled the x-label with ax1\n",
        "#     ax2.plot(final_vloss, color=color)\n",
        "#     ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "#     fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "#     plt.show()\n",
        "# except:\n",
        "#     pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "__SiZS4yg331"
      },
      "outputs": [],
      "source": [
        "# torch.save(net.state_dict(), 'network6.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVb51QoNeYjz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3d1b4IrSp6GJ"
      },
      "outputs": [],
      "source": [
        "# # Show outputs from trained net\n",
        "\n",
        "# print('initial state')\n",
        "# show_net(net, State())\n",
        "\n",
        "# print('WIN by put')\n",
        "# show_net(net, State().play('A1 C1 A2 C2'))\n",
        "\n",
        "# print('LOSE by opponent\\'s double')\n",
        "# show_net(net, State().play('B2 A2 A3 C1 B3'))\n",
        "\n",
        "# print('WIN through double')\n",
        "# show_net(net, State().play('B2 A2 A3 C1'))\n",
        "\n",
        "# # hard case: putting on A1 will cause double\n",
        "# print('strategic WIN by following double')\n",
        "# show_net(net, State().play('B1 A3'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Human Benchmarking\n"
      ],
      "metadata": {
        "id": "E-JfNr9b0Fdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load('network4.pkl'))\n",
        "net.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "kTlLSZkU0P6m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = Tree(net)\n",
        "print(benchmark(tree, eval_num=15, net_think_time=400, mcts_think_time=10))"
      ],
      "metadata": {
        "id": "pyn643QjvL3v",
        "outputId": "85a8e006-655a-4067-e262-b084b4ee1d6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 is net. Elos {0: 1000, 1: 1000}\n",
            "1 is net. Elos {0: 1016.0, 1: 984.736306793522}\n",
            "1 is net. Elos {0: 998.5641271217237, 1: 1001.3727654351186}\n",
            "1 is net. Elos {0: 1014.6934668785423, 1: 985.9859060052365}\n",
            "1 is net. Elos {0: 997.3744352524023, 1: 1002.5101794034746}\n",
            "1 is net. Elos {0: 981.6109277863209, 1: 1017.5488924496749}\n",
            "1 is net. Elos {0: 967.2600548690993, 1: 1031.2490445025305}\n",
            "1 is net. Elos {0: 986.1739837255413, 1: 1013.1848295781563}\n",
            "1 is net. Elos {0: 971.415379135847, 1: 1027.2704893102386}\n",
            "1 is net. Elos {0: 957.965668997536, 1: 1040.1205530518514}\n",
            "1 is net. Elos {0: 945.6800693002378, 1: 1051.8754492506673}\n",
            "1 is net. Elos {0: 966.4237337855413, 1: 1032.0177266586911}\n",
            "1 is net. Elos {0: 985.4090636507129, 1: 1013.8841022345284}\n",
            "1 is net. Elos {0: 970.7174594192295, 1: 1027.9063708084539}\n",
            "1 is net. Elos {0: 957.3275756510114, 1: 1040.7000806673677}\n",
            "1052.4051643607286\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3w2LWyl8p6GK",
        "outputId": "9b6fcda4-d150-4671-c114-9fa9c28e7549",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input move: C3\n",
            "Bot to play: \n",
            "  1 2 3 4 5\n",
            "A . . . . . \n",
            "B  . . . . . \n",
            "C   . . x . . \n",
            "D    . . . . . \n",
            "E     . . . . .  -1\n",
            "Max Value: -9.0 Rewards: defaultdict(<class 'float'>, {0: -10.0, 1: -10.0, 2: -10.0, 3: -10.0, 4: -10.0, 5: -10.0, 6: -10.0, 7: -10.0, 8: -9.0, 9: -9.0, 10: -9.0, 11: -9.0, 13: -9.0, 14: -9.0, 15: -9.0, 16: -9.0, 17: -9.0, 18: -9.0, 19: -9.0, 20: -9.0, 21: -9.0, 22: -9.0, 23: -9.0, 24: -9.0}) Moves: defaultdict(<class 'int'>, {0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 9, 9: 9, 10: 9, 11: 9, 13: 9, 14: 9, 15: 9, 16: 9, 17: 9, 18: 9, 19: 9, 20: 9, 21: 9, 22: 9, 23: 9, 24: 9})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Value: -16.0 Rewards: defaultdict(<class 'float'>, {0: -17.0, 1: -17.0, 2: -17.0, 3: -17.0, 4: -17.0, 5: -17.0, 6: -17.0, 7: -17.0, 8: -17.0, 9: -17.0, 10: -17.0, 11: -17.0, 13: -17.0, 14: -17.0, 15: -17.0, 16: -16.0, 17: -16.0, 18: -16.0, 19: -16.0, 20: -16.0, 21: -16.0, 22: -16.0, 23: -16.0, 24: -16.0}) Moves: defaultdict(<class 'int'>, {0: 17, 1: 17, 2: 17, 3: 17, 4: 17, 5: 17, 6: 17, 7: 17, 8: 17, 9: 17, 10: 17, 11: 17, 13: 17, 14: 17, 15: 17, 16: 16, 17: 16, 18: 16, 19: 16, 20: 16, 21: 16, 22: 16, 23: 16, 24: 16})\n",
            "[17]\n",
            " 1 2 3 4 5\n",
            "A . . . . . \n",
            "B  . . . . . \n",
            "C   . . x . . \n",
            "D    . . o . . \n",
            "E     . . . . . \n"
          ]
        }
      ],
      "source": [
        "# Search with trained net\n",
        "\n",
        "# tree = Tree(net)\n",
        "tree = MCTSAgent()\n",
        "state = State()\n",
        "while True:\n",
        "  user_input = input(\"Input move: \")\n",
        "  state.play(user_input)\n",
        "  if state.terminal():\n",
        "    break\n",
        "  distb = tree.think(state, 400, temperature=1, show=True)\n",
        "  pv_seq = tree.pv(state)\n",
        "  print(pv_seq)\n",
        "  state.play(pv_seq[0])\n",
        "  # display_heatmap2(distb.reshape((BOARD_SIZE,BOARD_SIZE)))\n",
        "  print(state)\n",
        "  if state.terminal():\n",
        "    break\n",
        "print(state)\n",
        "print(state.terminal_reward())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nmT85hx4TdB"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BotsForGames_Sprint2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}