{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miyamura80/BotsForGames/blob/main/BotsFightBots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSV6lARo71_n",
        "outputId": "f715f8ac-6b40-4292-b280-e58fb4582ad3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n",
            "Collecting open_spiel\n",
            "  Downloading open_spiel-1.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 7.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pip>=20.0.2 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (21.1.3)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (21.4.0)\n",
            "Collecting scipy>=1.5.4\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from open_spiel) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.10.0->open_spiel) (1.15.0)\n",
            "Installing collected packages: scipy, open-spiel\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed open-spiel-1.1.0 scipy-1.7.3\n"
          ]
        }
      ],
      "source": [
        "# environment:\n",
        "!pip3 install torch\n",
        "!pip install --upgrade open_spiel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZG_lS088nnu"
      },
      "source": [
        "# State Definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaC7HRUH8ayE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pyspiel\n",
        "import copy\n",
        "\n",
        "BOARD_SIZE = 5\n",
        "game = pyspiel.load_game(\"hex\",{\"board_size\":BOARD_SIZE})\n",
        "BLACK, WHITE = 1, -1  # first turn or second turn player\n",
        "\n",
        "class State:\n",
        "    '''Board implementation of BOARD_SIZE x BOARD_SIZE Hex Board'''\n",
        "    X, Y = 'ABCDEFGHI'[0:BOARD_SIZE],  '123456789'[0:BOARD_SIZE]\n",
        "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
        "\n",
        "    def __init__(self):\n",
        "        self.board = np.zeros((BOARD_SIZE, BOARD_SIZE)) # (x, y)\n",
        "        self.color = 1\n",
        "        self.win_color = 0\n",
        "        self.record = []\n",
        "        self.hex_state = game.new_initial_state()\n",
        "\n",
        "    def __deepcopy__(self):\n",
        "        newState = State()\n",
        "        newState.board = copy.deepcopy(self.board)\n",
        "        newState.win_color = copy.deepcopy(self.win_color)\n",
        "        newState.record = copy.deepcopy(self.record)\n",
        "        newState.hex_state = copy.deepcopy(self.hex_state)\n",
        "        return newState\n",
        "\n",
        "    def action2str(self, a: int):\n",
        "        return self.X[a // BOARD_SIZE] + self.Y[a % BOARD_SIZE]\n",
        "\n",
        "    def str2action(self, s: str):\n",
        "        return self.X.find(s[0]) * BOARD_SIZE + self.Y.find(s[1])\n",
        "\n",
        "    def record_string(self):\n",
        "        return ' '.join([self.action2str(a) for a in self.record])\n",
        "\n",
        "    def __str__(self):\n",
        "        final_bd = [\" \"+\" \".join(self.Y)]\n",
        "        hex_bd = str(self.hex_state).split(\"\\n\")\n",
        "        for i in range(len(hex_bd)):\n",
        "            final_bd.append(self.X[i]+\" \"+hex_bd[i])\n",
        "        return \"\\n\".join(final_bd)\n",
        "\n",
        "    def play(self, action):\n",
        "        # state transition function\n",
        "        # action is position interger (0~8) or string representation of action sequence\n",
        "        # Handles the case where action is sequence of actions \"0 1 2 3 4\"\n",
        "        if isinstance(action, str):\n",
        "            for astr in action.split():\n",
        "                self.play(self.str2action(astr))\n",
        "            return self\n",
        "\n",
        "        # Single action case\n",
        "        x, y = action // BOARD_SIZE, action % BOARD_SIZE\n",
        "        self.board[x, y] = self.color\n",
        "        self.hex_state.apply_action(action)\n",
        "\n",
        "        # check whether 3 stones are on the line\n",
        "        if self.hex_state.is_terminal():\n",
        "            self.win_color = self.color\n",
        "\n",
        "        self.color = -self.color\n",
        "        self.record.append(action)\n",
        "        return self\n",
        "\n",
        "    def terminal(self):\n",
        "        # terminal state check\n",
        "        return self.hex_state.is_terminal()\n",
        "\n",
        "    def terminal_reward(self):\n",
        "        # terminal reward \n",
        "        # return self.win_color if self.color == BLACK else -self.win_color\n",
        "        return self.win_color\n",
        "\n",
        "    def legal_actions(self):\n",
        "        # list of legal actions on each state\n",
        "        return [a for a in range(BOARD_SIZE * BOARD_SIZE) if self.board[a // BOARD_SIZE, a % BOARD_SIZE] == 0]\n",
        "\n",
        "    def feature(self):\n",
        "        # input tensor for neural net (state)\n",
        "        # return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
        "        observation =  np.array(self.hex_state.observation_tensor(), np.float32)\n",
        "        return observation.reshape(9,BOARD_SIZE,BOARD_SIZE)[1:BOARD_SIZE+1,:,:]\n",
        "\n",
        "    def action_feature(self, action):\n",
        "        # input tensor for neural net (action)\n",
        "        a = np.zeros((1, BOARD_SIZE, BOARD_SIZE), dtype=np.float32)\n",
        "        a[0, action // BOARD_SIZE, action % BOARD_SIZE] = 1\n",
        "        return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IBE2SVKSEju",
        "outputId": "9adbefd6-f3ba-4a48-cb84-8b10292785bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{}\n",
            "True\n",
            "0.3326408235594726\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "a = defaultdict(dict)\n",
        "print(a[\"he\"])\n",
        "print(\"he\" in a)\n",
        "a = {i: random.random() for i in range(10)}\n",
        "print(np.std(list(a.values())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EZVYO5a8tjw"
      },
      "source": [
        "# MCTS Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "9K6XEEO18v7f"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import random\n",
        "import time\n",
        "from math import sqrt, log\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(0)\n",
        "\n",
        "class MCTSAgent:\n",
        "    def __init__(self) -> None:\n",
        "        self.best = []\n",
        "        # Both of these :: path -> dict[move, x]\n",
        "        self.moves = defaultdict(lambda: defaultdict(int))\n",
        "        self.reward = defaultdict(lambda: defaultdict(float))\n",
        "    \n",
        "    def ucb_weight_general(self, state, mv, epoch, c=2.0):\n",
        "        path = state.record_string()\n",
        "        expected_reward = self.reward[path][mv]/(self.moves[path][mv]+1)\n",
        "        n_visit = self.moves[path][mv]\n",
        "        return expected_reward + c * sqrt(log(epoch)/(n_visit+1))\n",
        "\n",
        "    def think(self, state: State, sim_num: int, temperature:int, show=False) -> None:\n",
        "        if show:\n",
        "            print(\"Bot to play: \\n\", state, state.color)\n",
        "            uncertainties = []\n",
        "\n",
        "        start, prev_time = time.time(), 0        \n",
        "        if state.terminal():\n",
        "            return\n",
        "        \n",
        "        init_path = state.record_string()\n",
        "        for epoch in range(1, sim_num):\n",
        "            freshState = state.__deepcopy__()\n",
        "            # Display search result on every second\n",
        "            if show:\n",
        "                tmp_time = time.time() - start\n",
        "                if int(tmp_time) > int(prev_time):\n",
        "                    prev_time = tmp_time\n",
        "                    pv = self.pv(freshState)\n",
        "                    ucb_uncertainty = 2.0 * sqrt(log(epoch)/(self.moves[init_path][pv[0]]+1))\n",
        "                    uncertainties.append(ucb_uncertainty)\n",
        "                    print(f\"Uncertainty: {ucb_uncertainty}\")\n",
        "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d.'\n",
        "                          % (tmp_time, state.action2str(pv[0]), self.reward[init_path][pv[0]] / (self.moves[init_path][pv[0]]+1), \n",
        "                            self.moves[init_path][pv[0]], epoch))\n",
        "            not_terminated = True\n",
        "            rewards = []\n",
        "            while not_terminated:\n",
        "                # first_move = random.choice(list(self.moves))\n",
        "                path = freshState.record_string()\n",
        "                ucb_weights = [self.ucb_weight_general(freshState, k, epoch) for k in freshState.legal_actions()]\n",
        "                max_ucb_weight = max(ucb_weights)\n",
        "                move = [k for k in freshState.legal_actions() if self.ucb_weight_general(freshState, k, epoch)==max_ucb_weight][0]\n",
        "                if move in self.moves[path]:\n",
        "                  self.moves[path][move] += 1\n",
        "                else:\n",
        "                  self.moves[path][move] = 1\n",
        "                freshState.play(move)\n",
        "                if path not in self.reward:\n",
        "                  self.reward[path] = {move: 0}\n",
        "                rewards.append((self.reward[path], move))  \n",
        "                not_terminated = not freshState.terminal()\n",
        "            for (r,m) in rewards:\n",
        "                r[m] += freshState.terminal_reward()\n",
        "        if show:\n",
        "            plt.plot(uncertainties)\n",
        "            plt.show()\n",
        "\n",
        "    def pv(self, state: State) -> List[int]:\n",
        "        path = state.record_string()\n",
        "        if path in self.reward:\n",
        "          max_value = max(self.reward[path].values())\n",
        "          max_moves = [k for k,v in self.reward[path].items() if v==max_value]\n",
        "          print(f\"Max Value: {max_value} Rewards: {self.reward[path]} Moves: {self.moves[path]}\")\n",
        "        else:\n",
        "          max_moves = state.legal_actions()\n",
        "          print(\"ah\")\n",
        "        return [random.choice(max_moves)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxcEVp4v8ws3"
      },
      "source": [
        "# Test Bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvzmcwiS8a9Z",
        "outputId": "869f4fef-ec21-4f7f-dbdd-2e720908e900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Value: 284.0 Rewards: defaultdict(<class 'float'>, {0: 62.0, 1: 62.0, 2: 62.0, 3: 120.0, 4: 284.0, 5: 138.0, 6: 138.0, 7: 189.0, 8: 284.0, 9: 220.0, 10: 220.0, 11: 231.0, 12: 284.0, 13: 255.0, 14: 255.0, 15: 284.0, 16: 284.0, 17: 284.0, 18: 215.0, 19: 215.0, 20: 283.0, 21: 83.0, 22: 83.0, 23: 83.0, 24: 83.0}) Moves: defaultdict(<class 'int'>, {0: 86, 1: 86, 2: 86, 3: 140, 4: 284, 5: 158, 6: 158, 7: 203, 8: 284, 9: 230, 10: 230, 11: 239, 12: 284, 13: 259, 14: 259, 15: 284, 16: 284, 17: 284, 18: 225, 19: 225, 20: 283, 21: 107, 22: 107, 23: 107, 24: 107})\n",
            " 1 2 3 4 5\n",
            "A . . . . . \n",
            "B  . . . . . \n",
            "C   . . . . . \n",
            "D    x . . . . \n",
            "E     . . . . .  -1\n",
            "Input move: C3\n",
            "Max Value: 424.0 Rewards: defaultdict(<class 'float'>, {0: 134.0, 1: 134.0, 2: 89.0, 3: 89.0, 4: 303.0, 5: 198.0, 6: 198.0, 7: 198.0, 8: 302.0, 9: 252.0, 10: 136.0, 11: 162.0, 13: 265.0, 14: 265.0, 16: 386.0, 17: 328.0, 18: 315.0, 19: 315.0, 20: 424.0, 21: -5.0, 22: -5.0, 23: -5.0, 24: -5.0}) Moves: defaultdict(<class 'int'>, {0: 166, 1: 166, 2: 121, 3: 121, 4: 323, 5: 226, 6: 226, 7: 226, 8: 322, 9: 278, 10: 168, 11: 192, 13: 289, 14: 289, 16: 398, 17: 346, 18: 335, 19: 335, 20: 432, 21: 13, 22: 13, 23: 13, 24: 13})\n",
            " 1 2 3 4 5\n",
            "A . . . . . \n",
            "B  . . . . . \n",
            "C   . . o . . \n",
            "D    z . . . . \n",
            "E     z . . . .  -1\n",
            "Input move: B2\n",
            "Max Value: 433.0 Rewards: defaultdict(<class 'float'>, {0: 88.0, 1: 88.0, 2: 84.0, 3: 92.0, 4: 114.0, 5: 433.0, 7: 271.0, 8: 97.0, 9: 182.0, 10: 99.0, 11: 259.0, 13: 163.0, 14: 312.0, 16: 163.0, 17: 312.0, 18: 312.0, 19: 312.0, 21: 163.0, 22: 163.0, 23: 163.0, 24: 163.0}) Moves: defaultdict(<class 'int'>, {0: 132, 1: 132, 2: 128, 3: 136, 4: 160, 5: 477, 7: 321, 8: 143, 9: 230, 10: 145, 11: 309, 13: 211, 14: 360, 16: 211, 17: 360, 18: 360, 19: 360, 21: 211, 22: 211, 23: 211, 24: 211})\n",
            " 1 2 3 4 5\n",
            "A . . . . . \n",
            "B  x o . . . \n",
            "C   . . o . . \n",
            "D    z . . . . \n",
            "E     z . . . .  -1\n",
            "Input move: C1\n",
            "Max Value: 1271.0 Rewards: defaultdict(<class 'float'>, {0: -9.0, 1: -9.0, 2: -9.0, 3: -9.0, 4: 46.0, 7: 657.0, 8: 978.0, 9: 871.0, 11: 1271.0, 13: 46.0, 14: -8.0, 16: -8.0, 17: -8.0, 18: -8.0, 19: -8.0, 21: -8.0, 22: -8.0, 23: -8.0, 24: -8.0}) Moves: defaultdict(<class 'int'>, {0: 13, 1: 13, 2: 13, 3: 13, 4: 108, 7: 827, 8: 1188, 9: 1069, 11: 1507, 13: 108, 14: 16, 16: 16, 17: 16, 18: 16, 19: 16, 21: 16, 22: 16, 23: 16, 24: 16})\n",
            " 1 2 3 4 5\n",
            "A . . . . . \n",
            "B  x p . . . \n",
            "C   p z o . . \n",
            "D    z . . . . \n",
            "E     z . . . .  -1\n"
          ]
        }
      ],
      "source": [
        "agent = MCTSAgent()\n",
        "state = State()\n",
        "while True:\n",
        "  \n",
        "  distb = agent.think(state, 5000, temperature=1, show=False)\n",
        "  pv_seq = agent.pv(state)\n",
        "  state.play(pv_seq[0])\n",
        "\n",
        "  print(state, state.color)\n",
        "  if state.terminal():\n",
        "    break\n",
        "\n",
        "  while True:\n",
        "    user_input = input(\"Input move: \")\n",
        "    if state.str2action(user_input) in state.legal_actions():\n",
        "      break\n",
        "  state.play(user_input)\n",
        "  if state.terminal():\n",
        "    break\n",
        "print(state)\n",
        "print(state.terminal_reward())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "BotsFightBots.ipynb",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPBXWt/ycB65o/creknhutb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}